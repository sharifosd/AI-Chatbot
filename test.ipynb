{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "print(\"Loading saved model and tokenizer...\")\n",
    "MODEL_PATH = \"university_faq_model\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_PATH,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# Convert model for inference\n",
    "print(\"Preparing model for inference...\")\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Setup tokenizer with chat template\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def test_model(question, model, tokenizer):\n",
    "    # Format the conversation using the chat template\n",
    "    conversation = [\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize the formatted prompt\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test the model with a few examples\n",
    "print(\"\\nTesting the trained model:\")\n",
    "test_questions = [\n",
    "    \"What departments are available?\",\n",
    "    \"What is the semester fee for Computer Science?\",\n",
    "    \"কোন কোন ডিপার্টমেন্ট আছে?\",  # Bengali test\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(\"\\nQuestion:\", question)\n",
    "    response = test_model(question, model, tokenizer)\n",
    "    print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Combining Localhost server with ngrok\n",
    "from pyngrok import ngrok\n",
    "import os\n",
    "\n",
    "# Set your authtoken (replace with your actual token)\n",
    "ngrok.set_auth_token(\"2pEwqrKIFCT9Z5beoBPsmNFWjs2_24vXQkagVA29RdzYuBExr\")\n",
    "\n",
    "# Start ngrok tunnel\n",
    "public_url = ngrok.connect(8000)\n",
    "print(f\"\\nPublic URL: {public_url}\")\n",
    "print(\"You can now use this URL in Postman\")\n",
    "print(\"- GET /   (for status)\")\n",
    "print(\"- POST /generate   (for generating responses)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
